### To understand what data engineers do, why they are necessary and the impact they have, you need to know how data flows through an organization.

### The four steps of the data science workflow chronologically is: 

- Data collection and storage.
- Data preparation.
- Exploration and visualization.
- Experimentation and prediction.

#### You recently joined the data science team as a manager for a music streaming company named Spotflix. It's a music platform that lets users stream songs, create playlists, follow artists, watch music videos and even look up lyrics!

#### One of your colleagues just walked to your desk. They just got hired, but they already know you're on the data team - after training with DataCamp, you've made a name for yourself pretty quick! They have a bunch of data tasks they need completed, and they want to make sure they ask the right person. You tell them you can help them identify what they should request from data engineers, and what they should not.

| Data Engineering tracks                                                                   | Not data engineering tasks                                                      | 
| ------------------------------------------------------------------------------------------| --------------------------------------------------------------------------------| 
| Gather music consumption data from desktop and mobile sources.                            | Building a visualization to understand listening patterns by city.              |
| Optimize the customers databases for analysis.                                            | Running an experiment to identify the optimal search bar positioning in the app.
| Ensuring corrupted, unreadable muic tracks are removed and don't end up facing customers. | Based on their listening behaviour , predict which songs customers are likey to enjoy.


#### Companies ingest data from many different sources, which needs to be processed and stored in various ways. To handle that, we need data pipelines that efficiently automate the flow from one station to the next, so that data scientists can use up-to-date, accurate, relevant data. This isn't a simple task and that's why data engineers are so important.

#### The data is ingested into Spotflix's system, moving from their respective sources to our data lake.


#### Data pipelines ensure the data flows efficiently through the organization. They automate extracting, transforming, combining, validating, and loading data, to reduce human intervention and errors, and decrease the time it takes for data to flow through the organization.


## ETL and data pipelines.
***
#### One term you will hear a lot is `ETL`. It's a popular framework for designing data pipelines. It breaks up the flow of data into three sequential steps: 
- first `E` for `extracting` the data.
- `T` for `transforming` the data.
- `L` for `loading` this transformed data to a new database.

#### The key here is that data is processed before it's stored. In general, data pipelines move data from one system to another. They may follow ETL, but not all the time. For instance, the data may not be transformed, and routed directly to applications like visualization tools or Salesforce.