### What does it mean to `process` data? 
***

#### In a nutshell, data processing consists in converting `raw data` into `meaningful information`.

#### We want to move and organize data so it is easier for analysts to find what they need. The data is again processed to extract the metadata and store it in a database, for easy access by data analysts and data scientists. You may want your data to fit a certain schema or structure.

## How data engineers process data.
***

#### In terms of data processing, data engineers have different responsibilities. They perform `data manipulation`, `cleaning`, and `tidying` tasks that can be automated, and that will always need to be done, regardless of the analysis anyone wants to do with them.


#### For example, rejecting corrupt song files, or deciding what happens with missing metadata. What should we do when the genre is missing? Do we reject the file, do we leave the genre blank, or do we provide one by default? They also ensure that the data is stored in a sanely structured database, and create views on top of the database tables for easy access by analysts. 

#### `Views` are the output of a stored query on the data. For example, artist data and album data should be stored in separate tables in the database, but people will often want to work on these things together. That means data engineers need to create a view in the database combining both tables. 

#### Data engineers also `optimize` the performance of databases, for example by `indexing` the data so it's easier to retrieve.


## Scheduling data.
***
#### Scheduling can apply to any task we listed in the previous data processing lesson. To demonstrate scheduling, we will focus on updating tables and databases to keep things straightforward and easy to understand. Scheduling is the glue of a data engineering system. It holds each small piece and organizes how they work together, by running tasks in a specific order and resolving all dependencies correctly.

## Batches and streams.
***

#### Data can be ingested in `batches`, which means it's sent by _groups_ at _specific intervals_. Batch processing is often _cheaper_ because you can schedule it when resources aren't being used elsewhere, typically overnight. 


#### The data can also be `streamed`, which means individual data records are sent through the pipeline as soon as they are updated. For example, if a user signs up, they want to be able to use the service right away, so we need to write their profile to the database immediately.

#### Another example of batch vs. stream processing would be offline vs online listening. If a user listens online, Spotflix can stream parts of the song one after the other. If the user wants to save the song to listen offline, we need to batch all parts of the song together so they can save it. 

#### There's a third option called `real-time`, used for example in fraud detection, but for the sake of simplification and because streaming is almost always real-time, we will consider them to be the same in this course.

## Scheduling tools.
***
#### Some tools for scheduling are Apache Airflow, or Luigi.


## Parallel computing.
***
#### Parallel computing forms the basis of almost all modern data processing tools. It is important mainly for `memory concerns`, but also for `processing power`. When big data processing tools perform a processing task, they _split it up into several smaller subtasks_. These subtasks are then distributed over several computers.


#### One benefit of having multiple processing units is the extra processing power itself. Another benefit of parallel computing for big data relates to memory. Instead of needing to load all of the data in one computer's memory, you can partition the data and load the subsets into memory of different computers. That means the memory footprint per computer is relatively small. 



#### There can be some disadvantages to parallel computing though. Moving data incurs a cost. What's more, splitting a task into subtasks and merging the results of the subtasks back into one final result requires some communication between processes, which takes some additional time. So if the gains of splitting into subtasks are minimal, it may not be worth taking that risk.

## Cloud computing.
***

#### Companies would need to provide enough processing power for peak moments, and at quieter times, much of the processing power would remain unused. It's avoiding this waste of resources that makes `cloud computing` so appealing.

### Cloud computing for `data processing`.

#### In the cloud, we rent servers, and the rent is cheap: we don't get the same discount from our server salesperson that Amazon). We don't need a room to store them, and we use the resources we need, at the time we need them. Many companies moved to the cloud as a way of cost optimization. 

#### Also, the _closer_ the server is to the user, the _less latency_ they will experience when using our application. To serve a global customer base, we need servers all over the world.

### Cloud computing for `data storage`.

#### Another reason for using cloud computing is `database reliability`. Running a data-critical company, we have to prepare for the worst. A fire can break out in an on-premises data center. To be safe, we need to replicate our data at a different geographical location. 

### Cloud providers.

#### The three big players, in decreasing order of market share, are Amazon Web Services, Microsoft Azure,and Google Cloud.

#### For file storage, their respective services are AWS S3, Azure Blob Storage and Google Cloud Storage.

#### For computation,they are AWS EC2,Azure Virtual Machines and Google Compute Engine.

#### For databases,they are AWS RDS, Azure SQL Database and Google Cloud SQL.

### Multicloud.

#### You don't need to take all your cloud services from the same provider though. You can use several ones: it's called `multicloud`. 

#### It has some advantages, like reducing reliance a single vendor. It also optimizes costs, and might be necessary because of local laws.

#### It also allows you to militate against disasters. For example, in 2017, AWS had an outage, which broke the internet: among other companies impacted half of the top hundred retailers were down. 