## What are databases?
***
#### a database is "a usually large collection of data organized especially for rapid search and retrieval." There are few pieces of vital information in this definition. First, the database holds data. Second, databases organize data.

#### Lastly, databases help us quickly retrieve or search for data. The database management system or DBMS is usually in charge of this.

## Databases vs storage systems.
***
#### The main difference between databases and simple storage systems like file systems is the level of organization and the fact that the database management systems abstract away a lot of complicated data operations like search, replication and much more. File systems host less such functionality.


## SQL and NoSQL.
***
#### Another distinction we can make is the one between SQL and NoSQL. Generally speaking, in SQL databases, tables form the data. The database schema defines relations between these tables. We call SQL databases relational. For example, we could create one table for customers and another for orders. 


## Database schema.
***
#### The database schema defines the relationships and properties. Typical SQL databases are `MySQL` and `PostgreSQL`. On the other hand, NoSQL databases are called `non-relational`. NoSQL is often associated with unstructured, schemaless data. 

#### That's a misconception, as there are several types of NoSQL databases and they are not all unstructured. Two highly used NoSQL database types are key-value stores like `Redis` or `document databases` like `MongoDB`.

#### In `key-value stores`, the values are simple. Typical use cases are `caching` or `distributed configuration`. Values in a document database are `structured` or `semi-structured` objects, for example, a JSON object.

## The database schema.
***
#### A schema describes the structure and relations of a database. 

## What is parallel computing?

#### Parallel computing forms the basis of almost all modern data processing tools. However, why has it become so important in the world of big data? The main reason is memory and processing power, but mostly memory. When big data processing tools perform a processing task, they split it up into several smaller subtasks. 

#### The processing tools then distribute these subtasks over several computers. These are usually commodity computers, which means they are widely available and relatively inexpensive. Individually, all of the computers would take a long time to process the complete task. However, since all the computers work in parallel on smaller subtasks, the task in its whole is done faster.

### Benefits of parallel computing.
***
#### The obvious benefit of having multiple processing units is the extra processing power itself. However, there is another, and potentially more impactful benefit of parallel computing for big data. Instead of needing to load all of the data in one computer's memory, you can partition the data and load the subsets into memory of different computers. That means the memory footprint per computer is relatively small, and the data can fit in the memory closest to the processor, the RAM.

### Risks of parallel computing.
***
#### Splitting a task into subtask and merging the results of the subtasks back into one final result requires some communication between processes. This communication overhead can become a bottleneck if the processing requirements are not substantial, or if you have too little processing units.

## Parallel computation frameworks.
***
### Hadoop.

#### Hadoop is a collection of open source projects, maintained by the Apache Software Foundation. Some of them are a bit outdated, but it's still relevant to talk about them. There are two Hadoop projects we want to focus on for this video: MapReduce and HDFS.

### HDFS
***
#### HDFS is a distributed file system. It's similar to the file system you have on your computer, the only difference being the files reside on multiple different computers. HDFS has been essential in the big data world, and for parallel computing by extension. Nowadays, cloud-managed storage systems like Amazon S3 often replace HDFS.

### MapReduce
***
####  For MapReduce, these processing units are several computers in the cluster. MapReduce had its flaws; one of it was that it was hard to write these MapReduce jobs. Many software programs popped up to address this problem, and one of them was Hive.

### Hive.

### Spark.
***
#### While MapReduce-based systems tend to need expensive disk writes between jobs, Spark tries to keep as much processing as possible in memory. In that sense, Spark was also an answer to the limitations of MapReduce. 

#### The disk writes of MapReduce were especially limiting in interactive exploratory data analysis, where each step builds on top of a previous step. Spark originates from the University of California, where it was developed at the Berkeley's AMPLab. Currently, the Apache Software Foundation maintains the project.




## Workflow scheduling frameworks.

### An example pipeline.
#### Let's take an example. You can write a Spark job that pulls data from a CSV file, filters out some corrupt records, and loads the data into a SQL database ready for analysis. However, let's say you need to do this every day as new data is coming in to the CSV file. One option is to run the job each day manually. 

#### Of course, that doesn't scale well: what about the weekends? There are simple tools that could solve this problem, like `cron`, the Linux tool. However, let's say you have one job for the CSV file and another job to pull in and clean the data from an API, and a third job that joins the data from the CSV and the API together. 

#### The third job depends on the first two jobs to finish. It quickly becomes apparent that we need a more holistic approach, and a simple tool like cron won't suffice.

### DAGs.
***
##### we ended up with dependencies between jobs. A great way to visualize these dependencies is through Directed Acyclic Graphs, or DAGs. A DAG is a set of nodes that are connected by directed edges. There are no cycles in the graph, which means that no path following the directed edges sees a specific node more than once.


#### The tools for the job.
***
#### People use the Linux tool, cron. However, most companies use a more full-fledged solution. There's Spotify's Luigi, which allows for the definition of DAGs for complex pipelines.